{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#provaxxers\n",
    "#provaxxersTweets = pd.read_csv('./datasets/provaxxers/provaxxersTweets.csv', low_memory=False)\n",
    "#vacinaja = pd.read_csv('./datasets/provaxxers/vacinaja.csv', low_memory=False)\n",
    "\n",
    "#antivaxxers\n",
    "#antivaxxersTweets = pd.read_csv('./datasets/antivaxxers/antivaxxersTweets.csv', low_memory=False)\n",
    "#antisinovaxxers\n",
    "#antisinovaxxersTweets = pd.read_csv('./datasets/antisinovaxxers/old/antisinovaxxersTweets.csv', low_memory=False)\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np \n",
    "from string import punctuation\n",
    "import unidecode\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "\n",
    "# Removendo hashtags, menções a usuários, numeros, termos curtos e links\n",
    "\n",
    "def proccess_text(raw_text):\n",
    "    \n",
    "    twitterData = pd.DataFrame(raw_text) \n",
    "    twitterData['processed_text'] = twitterData.text.str.lower() \\\n",
    "                                                    .str.replace(r'(http\\S+)', '') \\\n",
    "                                                    .str.replace(r'@[\\w]*', '') \\\n",
    "                                                    .str.replace(r'#[\\w]*','') \n",
    "\n",
    "    textWords = ' '.join([text for text in twitterData.processed_text])\n",
    "\n",
    "    # Removendo acentuação\n",
    "    textWords = [unidecode.unidecode(text) for text in twitterData.processed_text ]\n",
    "\n",
    "    # Criando lista com palavras e caracteres (stopwords) a serem removidos do texto\n",
    "    stopWords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "\n",
    "    # Separando a pontuação das palavras\n",
    "    punctSeparator = tokenize.WordPunctTokenizer()\n",
    "    punctuationList = list()\n",
    "    for punct in punctuation:\n",
    "        punctuationList.append(punct)\n",
    "\n",
    "    personalList=['predictions']    \n",
    "\n",
    "    stopWords =  stopWords + punctuationList + personalList\n",
    "    #stopWords =   punctuationList + personalList\n",
    "\n",
    "\n",
    "    # Iterando o texto removendo as stopwords\n",
    "    trasnformedText = list()\n",
    "    \n",
    "    for text in textWords:\n",
    "        newText = list()   \n",
    "        #text = text.lower()\n",
    "        textWords = punctSeparator.tokenize(text)\n",
    "        for words in textWords:\n",
    "             if words not in stopWords:\n",
    "                #newText.append(stemmer.stem(words))\n",
    "                newText.append(words)\n",
    "        trasnformedText.append(' '.join(newText))\n",
    "    twitterData.processed_text = trasnformedText\n",
    "\n",
    "   \n",
    "    twitterData.processed_text = twitterData.processed_text.str.replace(r\"[^a-zA-Z#]\", \" \") \\\n",
    "                                                           .replace(r\"k\\k\", \" \") #\\\n",
    "                                                           #.apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3])) \n",
    "    \n",
    "    return twitterData.processed_text\n",
    "\n",
    "\n",
    "tweets = {'created_at': antisinovaxxersTweets.created_at, 'text': antisinovaxxersTweets.text}\n",
    "rawTweets = pd.DataFrame(tweets)\n",
    "\n",
    "rawTweets['processed_text'] = proccess_text(rawTweets.text)\n",
    "\n",
    "processedTweets = rawTweets.drop(columns=[\"text\"])\n",
    "\n",
    "trasnformedText = list()\n",
    "for phrase in processedTweets.processed_text:\n",
    "    newPhrase = list()   \n",
    "    newPhrase.append(' '.join(phrase.split()))\n",
    "    for words in newPhrase:\n",
    "        trasnformedText.append(''.join(newPhrase))\n",
    "processedTweets.processed_text = trasnformedText\n",
    "\n",
    "index=[x for x in processedTweets.index if processedTweets.processed_text[x].count(' ') < 4]\n",
    "processedTweets = processedTweets.drop(index)\n",
    "\n",
    "removeEmpty  = processedTweets.processed_text != ' '\n",
    "processedTweets = processedTweets[removeEmpty]\n",
    "\n",
    "processedTweets.reset_index(inplace=True)\n",
    "\n",
    "tweets = {'created_at': processedTweets.created_at, 'text': processedTweets.processed_text}\n",
    "docs = pd.DataFrame(tweets)\n",
    "\n",
    "docs = docs.sort_values(['created_at']).reset_index()\n",
    "\n",
    "docs = docs.drop(columns=[\"index\"])\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "provaxxersTweets = pd.read_csv('./datasets/provaxxers/provaxxersTweets.csv', low_memory=False)\n",
    "oldProvaxxersTweets = pd.read_csv('./datasets/provaxxers/old/provaxxersTweets.csv', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date ='2020-02-29T00:00:00.000Z'\n",
    "end_date = '2021-05-04T00:00:00.000Z'\n",
    "\n",
    "mask = (provaxxersTweets['created_at'] > start_date) & (provaxxersTweets['created_at'] <= end_date)\n",
    "filteredTweets = provaxxersTweets.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():        \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")    \n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())    \n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6462dc2e",
   "metadata": {},
   "source": [
    "## Computing Sentence Embeddings with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b33547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "def computing_embeddings(docs):\n",
    "    bert_model = SentenceTransformer(\"xlm-r-bert-base-nli-stsb-mean-tokens\", device=\"cuda\")\n",
    "    bert_embeddings = bert_model.encode(docs, show_progress_bar=True,convert_to_tensor=False)\n",
    "    return bert_model, bert_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model, bert_embeddings = computing_embeddings(docs.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8daa23",
   "metadata": {},
   "source": [
    "## Dimensionality reduction with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed4fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "def reduce_dimensionality(embeddings,n,c):\n",
    "    umap_model = UMAP(n_neighbors=n, n_components=c, metric='cosine')\n",
    "    umap_embeddings = umap_model.fit_transform(embeddings)\n",
    "    return umap_model, umap_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f99c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model, umap_embeddings = reduce_dimensionality(bert_embeddings,15,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124ffe0",
   "metadata": {},
   "source": [
    "## Clustering with K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9446bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#kmeans = KMeans(n_clusters=40, random_state=0).fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d83e709",
   "metadata": {},
   "source": [
    "## Clustering with AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d907609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_model = AgglomerativeClustering(n_clusters=None,\n",
    "                                   affinity='cosine', \n",
    "                                   memory=None, \n",
    "                                   connectivity=None, \n",
    "                                   compute_full_tree='auto', \n",
    "                                   linkage='average', \n",
    "                                   distance_threshold=0.04, \n",
    "                                   compute_distances=True).fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0951bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_cluster = ac_model.fit_predict(umap_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cada12",
   "metadata": {},
   "source": [
    "## Clustering with Fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e025bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.cluster.hierarchy import ward, fcluster\n",
    "#from scipy.spatial.distance import pdist\n",
    "\n",
    "#Z = ward(pdist(umap_embeddings))\n",
    "#clusters_fc = fcluster(Z, t=0.9, criterion='distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e8f7f",
   "metadata": {},
   "source": [
    "## Clustering with HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea23516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "def custering_hdbscan(embeddings,min_cluster_size):\n",
    "    hdbsan_cluster = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, \n",
    "                              metric='euclidean',                      \n",
    "                              cluster_selection_method='eom', prediction_data=True).fit_predict(embeddings)\n",
    "    \n",
    "    return hdbsan_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbsan_cluster = custering_hdbscan(umap_embeddings,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5fc311",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.Series(hdbsan_cluster)\n",
    "c = c.unique()\n",
    "len(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930db897",
   "metadata": {},
   "source": [
    "# Ploting clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf2d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding_umap = UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079cfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ploting_clusters(embeddings,labels):\n",
    "    result = pd.DataFrame(embeddings, columns=['x', 'y'])\n",
    "    result['labels'] = labels\n",
    "    # Visualize clusters\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    outliers = result.loc[result.labels == -1, :]\n",
    "    clustered = result.loc[result.labels != -1, :]\n",
    "    plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.15)\n",
    "    plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781d1b87",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d31bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ploting_clusters(plot_embedding_umap,kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3b9093",
   "metadata": {},
   "source": [
    "### AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b88fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ploting_clusters(plot_embedding_umap,ac_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69673c96",
   "metadata": {},
   "source": [
    "###  HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74950465",
   "metadata": {},
   "outputs": [],
   "source": [
    "ploting_clusters(plot_embedding_umap,hdbsan_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab34a3a",
   "metadata": {},
   "source": [
    "# c-TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_documents(docs,cluster_labels):\n",
    "    docs_df = pd.DataFrame(docs, columns=[\"text\"])\n",
    "    docs_df['cluster'] = cluster_labels\n",
    "    docs_df['doc_id'] = range(len(docs_df))\n",
    "    docs_per_topic = docs_df.groupby(['cluster'], as_index = False).agg({'text': ' '.join})\n",
    "    return docs_df, docs_per_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df, docs_per_topic = clustering_documents(docs,hdbsan_cluster)\n",
    "#docs_df, docs_per_topic = clustering_documents(docs,ac_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d931c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range).fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "  \n",
    "c_tf_idf, count = c_tf_idf(docs_per_topic.text.values, m=len(docs))\n",
    "\n",
    "def extract_top_n_words_per_topic(c_tf_idf, count, docs_per_topic, n=10):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.cluster)\n",
    "    tf_idf_transposed = c_tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return words, top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['cluster'])\n",
    "                     .text\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"cluster\": \"topic\", \"text\": \"size\"}, axis='columns')\n",
    "                     .sort_values(\"size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "words,top_n_words = extract_top_n_words_per_topic(c_tf_idf, count, docs_per_topic, n=5)\n",
    "topic_sizes = extract_topic_sizes(docs_df); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a32513",
   "metadata": {},
   "source": [
    "# Maximal Marginal Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5965e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dd296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics= pd.Series(hdbsan_cluster)\n",
    "topics = topics.unique()\n",
    "topics  = topics = sorted(list(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = c_tf_idf.argsort()[:, -30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fe221",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {label: [(words[j], c_tf_idf[i][j])\n",
    "                          for j in indices[i]][::-1]\n",
    "                  for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58652cdc",
   "metadata": {},
   "source": [
    "### Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def visualize_heatmap(topic_model,\n",
    "                      topics: List[int] = None,\n",
    "                      top_n_topics: int = None,\n",
    "                      n_clusters: int = None,\n",
    "                      width: int = 800,\n",
    "                      height: int = 800) -> go.Figure:\n",
    "\n",
    "    # Select topic embeddings\n",
    "    if topic_model is not None:\n",
    "        embeddings = np.array(topic_model)\n",
    "    else:\n",
    "        embeddings = c_tf_idf\n",
    "\n",
    "\n",
    "    # Select topics based on top_n and topics args\n",
    "    topics= pd.Series(topics)\n",
    "    topics = topics.unique()\n",
    "    topics = sorted(list(topics))\n",
    "    \"\"\"\n",
    "     if topics is not None:\n",
    "        topics= pd.Series(topics)\n",
    "        topics = topics.unique()\n",
    "        topics = list(topics)\n",
    "    elif top_n_topics is not None:\n",
    "        topics = sorted(topics[1:top_n_topics + 1])\n",
    "    else:\n",
    "        #topics= pd.Series(topics)\n",
    "        #topics = topics.unique()\n",
    "        #topics = sorted(list(topics))\n",
    "   \n",
    "   \n",
    "\n",
    "        \n",
    "    # Order heatmap by similar clusters of topics\n",
    "    \n",
    "    if n_clusters:\n",
    "        if n_clusters >= len(set(topics)):\n",
    "            raise ValueError(\"Make sure to set `n_clusters` lower than \"\n",
    "                             \"the total number of unique topics.\")\n",
    "\n",
    "        embeddings = embeddings[[topic + 1 for topic in topics]]\n",
    "        distance_matrix = cosine_similarity(embeddings)\n",
    "        Z = linkage(distance_matrix, 'ward')\n",
    "        clusters = fcluster(Z, t=n_clusters, criterion='maxclust')\n",
    "\n",
    "        # Extract new order of topics\n",
    "        mapping = {cluster: [] for cluster in clusters}\n",
    "        for topic, cluster in zip(topics, clusters):\n",
    "            mapping[cluster].append(topic)\n",
    "        mapping = [cluster for cluster in mapping.values()]\n",
    "        sorted_topics = [topic for cluster in mapping for topic in cluster]\n",
    "    else:\n",
    "        sorted_topics = topics\n",
    "     \"\"\"   \n",
    "    sorted_topics = topics\n",
    "    # Select embeddings\n",
    "    indices = np.array([topics.index(topic) for topic in sorted_topics])\n",
    "    #print(indices)\n",
    "    embeddings = embeddings[indices]\n",
    "    distance_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Create nicer labels\n",
    "    #print(index)\n",
    "    #print(sorted_topics)\n",
    "\n",
    "    new_labels = [[[str(topic), None]] + top_n_words[topic] for topic in sorted_topics]\n",
    "    new_labels = [\"_\".join([label[0] for label in labels[:4]]) for labels in new_labels]\n",
    "    new_labels = [label if len(label) < 30 else label[:27] + \"...\" for label in new_labels]\n",
    "  \n",
    "    fig = px.imshow(distance_matrix,\n",
    "                    labels=dict(color=\"Similarity Score\"),\n",
    "                    x=new_labels,\n",
    "                    y=new_labels,\n",
    "                    color_continuous_scale='GnBu'\n",
    "                    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': \"<b>Similarity Matrix\",\n",
    "            'y': .95,\n",
    "            'x': 0.55,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top',\n",
    "            'font': dict(\n",
    "                size=22,\n",
    "                color=\"Black\")\n",
    "        },\n",
    "        width=width,\n",
    "        height=height,\n",
    "        hoverlabel=dict(\n",
    "            bgcolor=\"white\",\n",
    "            font_size=16,\n",
    "            font_family=\"Rockwell\"\n",
    "        ),\n",
    "    )\n",
    "    fig.update_layout(showlegend=True)\n",
    "    fig.update_layout(legend_title_text='Trend')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e31723",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_heatmap(topic_model=c_tf_idf,topics=hdbsan_cluster,width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_heatmap(topic_model=bert_embeddings,topics=ac_cluster,width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b90bd54",
   "metadata": {},
   "source": [
    "## BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb5f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "bert_model = SentenceTransformer(\"xlm-r-bert-base-nli-stsb-mean-tokens\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ffe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9690c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_model=hdbscan.HDBSCAN(min_cluster_size=24, \n",
    "                              metric='euclidean',                      \n",
    "                               cluster_selection_method='eom',prediction_data=True).fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412dc8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic(embedding_model=bert_model,\n",
    "                       #language = \"multilingual\",\n",
    "                       top_n_words=5,\n",
    "                       #n_gram_range=(1, 2),\n",
    "                       #min_topic_size=15,   \n",
    "                       #nr_topics = 10,\n",
    "                       umap_model=umap_model,\n",
    "                       #hdbscan_model=hdbscan_model,\n",
    "                       low_memory=True,\n",
    "                       calculate_probabilities=False, \n",
    "                       verbose=True)#.fit(processedTweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, _ = topic_model.fit_transform(docs.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "topic_model.save('./models/provaxxers-model')\n",
    "\n",
    "# Load model\n",
    "#my_model = BERTopic.load(\"my_model\")\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde34733",
   "metadata": {},
   "source": [
    "## Topic Reduction\n",
    "\n",
    "<ul>\n",
    "    <li>Manual Topic Reduction</li>\n",
    "    <li>Automatic Topic Reduction</li>\n",
    "    <li><b>Topic Reduction after Training</b></li>\n",
    "</ul> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98566a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "newTopics, newProbs = topic_model.reduce_topics(docs.text, topics,  nr_topics= 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa335e2",
   "metadata": {},
   "source": [
    "# Dynamic Topic Modeling\n",
    "\n",
    "[Dou et al. 2012] define um evento como ”uma ocorrencia que causa mudancas no volume de dados de texto que discutem o topico associado em um momento especifico. Logo, eventos podem ser vistos como um resumo sucinto dos fluxos de informacoes nas mıdias sociais, revelando a evolucao de fenomenos sociais ao longo de um determinado periodo de tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0c37dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = docs.created_at.to_list()\n",
    "tweets = docs.text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aecaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(docs=tweets, \n",
    "                                                topics=newTopics, \n",
    "                                                #topics=topics,\n",
    "                                                timestamps=timestamps, \n",
    "                                                global_tuning=False,\n",
    "                                                evolution_tuning=True, \n",
    "                                                nr_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f6ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ef27e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
