{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coleta dados do Twitter: Full-archive search \n",
    "\n",
    "Documentação da API: https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all \n",
    "\n",
    "Endpoint URL: https://api.twitter.com/2/tweets/search/all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "auth_token = os.environ.get('AUTH_TOKEN')\n",
    "\n",
    "header = {'Authorization': 'Bearer ' + auth_token}\n",
    "\n",
    "#query='((vacina%20vacinacao)%20OR%20(vacina%20OR%20vacinacao))%20-rt'\n",
    "query='%23cpidacovid%20-rt' # Query com o filtro -rt, para não trazer retweets \n",
    "start_time='2020-02-29T00%3A00%3A00Z'\n",
    "end_time='2021-07-12T00%3A00%3A00Z'\n",
    "#start_time='2021-05-03T00%3A00%3A00Z'\n",
    "#end_time='2021-01-18T11:26:54.000Z'\n",
    "\n",
    "max_results='500'\n",
    "next_token=''\n",
    "\n",
    "url='https://api.twitter.com/2/tweets/search/all?query='+query+'&start_time='+start_time+'&end_time='+end_time+'&max_results='+max_results+'&expansions=author_id&tweet.fields=created_at'\n",
    "response = requests.get(url,headers=header)\n",
    "time.sleep(1)\n",
    "listOfTweets = json.loads(response.content)\n",
    "print('New Request on',url)\n",
    "\n",
    "twitterData = pd.DataFrame(listOfTweets['data'])   \n",
    "#twitterUsers = pd.DataFrame(listOfTweets['includes'])\n",
    "\n",
    "if 'next_token' in listOfTweets['meta']:    \n",
    "    next_token = listOfTweets['meta']['next_token']   \n",
    "    \n",
    "    while 'next_token' in listOfTweets['meta']:        \n",
    "        url='https://api.twitter.com/2/tweets/search/all?query='+query+'&start_time='+start_time+'&end_time='+end_time+'&max_results='+max_results+'&next_token='+next_token+'&expansions=author_id&tweet.fields=created_at'\n",
    "        response = requests.get(url,headers=header)  \n",
    "        time.sleep(1)\n",
    "        listOfTweets = json.loads(response.content)         \n",
    "       \n",
    "        print('New Request on',url)\n",
    "        \n",
    "        if 'data' in listOfTweets:\n",
    "            twitterData = twitterData.append(pd.DataFrame(listOfTweets['data']),ignore_index=True)\n",
    "            #twitterUsers = twitterUsers.append(pd.DataFrame(listOfTweets['includes']),ignore_index=True)\n",
    "\n",
    "            if  'meta' in listOfTweets:         \n",
    "                if 'next_token' in listOfTweets['meta']:\n",
    "                    next_token =  listOfTweets['meta']['next_token']\n",
    "                else:\n",
    "                    print('Done! Total of ', len(twitterData), 'tweets collected.')                \n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            print('Missing request')\n",
    "            break\n",
    "else:\n",
    "    twitterData = pd.DataFrame(listOfTweets['data'])\n",
    "    #twitterUsers = pd.DataFrame(listOfTweets['includes'])\n",
    "\n",
    "    print('Done! Total of', len(twitterData), 'tweets collected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitterData.to_csv('tweets.csv')\n",
    "twitterData.to_csv('./datasets/cpi.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "twitterData = pd.read_csv('./datasets/old/allpandemic.csv', lineterminator='\\n',low_memory=False)\n",
    "\n",
    "reawTweets = {'created_at': twitterData.created_at, 'text': twitterData.text}\n",
    "tweets = pd.DataFrame(reawTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento dos dados textuais extraidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np \n",
    "from string import punctuation\n",
    "import unidecode\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "\n",
    "# Removendo hashtags, menções a usuários, numeros, termos curtos e links\n",
    "\n",
    "twitterData['processed_text'] = twitterData.text.str.lower() \\\n",
    "                                                .str.replace(r'(http\\S+)', '') \\\n",
    "                                                .str.replace(r'@[\\w]*', '') \\\n",
    "                                                .str.replace(r'#[\\w]*','') \n",
    "                        \n",
    "textWords = ' '.join([text for text in twitterData.processed_text])\n",
    "\n",
    "# Removendo acentuação\n",
    "textWords = [unidecode.unidecode(text) for text in twitterData.processed_text ]\n",
    "\n",
    "# Criando lista com palavras e caracteres (stopwords) a serem removidos do texto\n",
    "stopWords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "\n",
    "# Separando a pontuação das palavras\n",
    "punctSeparator = tokenize.WordPunctTokenizer()\n",
    "punctuationList = list()\n",
    "for punct in punctuation:\n",
    "    punctuationList.append(punct)\n",
    "    \n",
    "personalList=['pra','predictions']    \n",
    "\n",
    "stopWords =  stopWords + punctuationList + personalList\n",
    "\n",
    "# Iterando o texto removendo as stopwords\n",
    "trasnformedText = list()\n",
    "\n",
    "for text in textWords:\n",
    "    newText = list()   \n",
    "    #text = text.lower()\n",
    "    textWords = punctSeparator.tokenize(text)\n",
    "    for words in textWords:\n",
    "        if words not in stopWords:\n",
    "             #newText.append(stemmer.stem(words))\n",
    "            newText.append(words)\n",
    "    trasnformedText.append(' '.join(newText))\n",
    "twitterData.processed_text = trasnformedText\n",
    "\n",
    "twitterData.processed_text = twitterData.processed_text.str.replace(r\"[^a-zA-Z#]\", \" \") \\\n",
    "                                                           .replace(r\"k\\k\", \" \") \\\n",
    "                                                           .apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualização dos dados textuais pré-processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitterData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterData.processed_text[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterData.text[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "textWords = ' '.join([text for text in  twitterData.processed_text])\n",
    "\n",
    "tokenizing = tokenize.WhitespaceTokenizer()\n",
    "tokenizedWords = tokenizing.tokenize(textWords)\n",
    "frequency = nltk.FreqDist(tokenizedWords)\n",
    "\n",
    "df_frequency = pd.DataFrame({\"Word\": list(frequency.keys()),\n",
    "                                   \"Frequency\": list(frequency.values())})\n",
    "\n",
    "df_frequency.nlargest(columns = \"Frequency\", n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pareto(text, text_column, count):\n",
    "    textWords = ' '.join([text for text in text[text_column]])\n",
    "    tokenizedWords = tokenizing.tokenize(textWords)\n",
    "    frequency = nltk.FreqDist(tokenizedWords)\n",
    "    df_frequency = pd.DataFrame({\"Word\": list(frequency.keys()),\n",
    "                                   \"Fequency\": list(frequency.values())})\n",
    "    df_frequency = df_frequency.nlargest(columns = \"Fequency\", n = count)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = sns.barplot(data =  df_frequency, x = \"Word\", y = \"Fequency\", color = 'gray')\n",
    "    ax.set(ylabel = \"Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto(twitterData, \"processed_text\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "\n",
    "textWords = ' '.join([text for text in  twitterData.processed_text ])\n",
    "\n",
    "wordCloud = WordCloud(width= 800, height= 600,  #WordCloud com a lista de palavas\n",
    "                          max_font_size = 110,\n",
    "                          collocations = False).generate(textWords)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordCloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF / LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from tqdm import tqdm\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):        \n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "processed_docs = twitterData.processed_text.map(preprocess)\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções para ajustar os hiperpârametros do modelo \n",
    "\n",
    "<ul>\n",
    "    <li><b>coherence:</b> valor de coerência mede, dentro de cada tópico, a coerência semântica das palavras, utilizando a métrica de cossenos para                  determinar sua similaridade.Varia de 0 a 1.  </li>\n",
    "    <li><b>num_topics:</b> numeros de tópicos</li>\n",
    "    <li><b>alpha:</b> desnidade de tópicos no documento</li>\n",
    "    <li><b>beta:</b> densidade de palavras no tópico</li>\n",
    "</ul> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in tqdm(range(start, limit, step)):\n",
    "        model = LdaMulticore(corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "# Colocando parametros na função\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus_tfidf, texts=processed_docs, start=1, limit=30, step=1)\n",
    "# Mostrando visualmente a quantidade de tópicos\n",
    "limit=30; start=1; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Tópicos\")\n",
    "plt.ylabel(\"Score de Coerência\")\n",
    "plt.legend((\"Valores de Coerência\"), loc='best')\n",
    "plt.show()\n",
    "# Lista dos valores de coerência, para melhor identificar o ponto de inflexão do gráfico\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"A quantidade de tópicos =\", m, \" tem um valor de coerência de \", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus_tfidf,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()\n",
    "\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "print()\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus_tfidf)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               gensim.utils.ClippedCorpus(corpus_tfidf, num_of_docs*0.75), \n",
    "               corpus_tfidf]\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=20)\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_tfidf[i], dictionary=dictionary, k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando LDA com os hiperparâmetros sugeridos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = LdaMulticore(corpus_tfidf, id2word=dictionary, num_topics=12 , passes=100,chunksize = 1000, workers=4, per_word_topics=True,\n",
    "                        alpha = 0.9,\n",
    "                        eta = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model_tfidf.print_topics( num_words = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda  = CoherenceModel(model=lda_model_tfidf, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nScore de Coerência: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPerplexidade: ', lda_model_tfidf.log_perplexity(corpus_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "\n",
    "cloud = WordCloud( background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=20,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "topics = lda_model_tfidf.show_topics(formatted=False)\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15,15), sharex=True, sharey=True)\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=600)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_tfidf, corpus_tfidf, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in tqdm(range(start, limit, step)):\n",
    "        model = LdaMulticore(corpus, id2word=dictionary, num_topics=4)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "# Colocando parametros na função\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus_tfidf, texts=processed_docs, start=1, limit=4, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escolhe o model\n",
    "optimal_model = model_list[0]\n",
    "optimal_model.print_topics( num_words = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar qual o principal tópico em cada tweet\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "# Seleciona o principal tópico de cada tweet\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Calcula o principal tópico, o percentual de contribuição e as palavras chaves de cada tweet\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => mostra o principal tópico\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Principal_Topico', 'Perc_Contributicao', 'Palavras_Chave']\n",
    "# Inclui o texto original no final do DataFrame\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "# roda a função de calcular os principais tópicos de cada tweet\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus_tfidf, texts=processed_docs)\n",
    "# Formata o DataFrame\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Index_DF_Original', 'Principal_Topico', 'Perc_Contrib_Topico', 'Palavras_Chave', 'Tweets']\n",
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Principal_Topico')\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contributicao'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['topic', \"topic_perc_contrib\", \"terms\", \"tweet\"]\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantidade de tweets por tópico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de tweets por topico\n",
    "topic_counts = df_topic_sents_keywords['Principal_Topico'].value_counts()\n",
    "# Porcentagem de tweets por tópico\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "# Número do tópico e suas palavras chave\n",
    "topic_num_keywords = sent_topics_sorteddf_mallet[['topic', 'terms']]\n",
    "# Concatena as colunas\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "# Renomeia as colunas\n",
    "df_dominant_topics.columns = ['topics', 'terms', 'num_tweets', 'perc_tweets']\n",
    "# Mostra o DataFrame\n",
    "df_dominant_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3), subplot_kw=dict(aspect=\"equal\"))\n",
    "\n",
    "\n",
    "topics = df_dominant_topics.topics\n",
    "print(topics)\n",
    "keywords = df_dominant_topics.terms\n",
    "\n",
    "\n",
    "def func(pct, allvals):\n",
    "    absolute = int(pct/100.*np.sum(allvals))\n",
    "    return \"{:.1f}%\\n Topic {:d}\".format(pct, absolute)\n",
    "\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(topics, radius=2,autopct=lambda pct: func(pct, topics),\n",
    "                                  textprops=dict(color=\"w\"))\n",
    "\n",
    "ax.legend(wedges, keywords,\n",
    "          title=\"Terms\",\n",
    "          loc=\"center left\",\n",
    "          bbox_to_anchor=(1.5, -0.5, 0.5, 1))\n",
    "\n",
    "plt.setp(autotexts, size=12, weight=\"bold\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "\n",
    "cloud = WordCloud( background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=20,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "topics = optimal_model.show_topics(formatted=False)\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15,15), sharex=True, sharey=True)\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=600)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
