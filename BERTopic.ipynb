{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#provaxxers\n",
    "#provaxxersTweets = pd.read_csv('./datasets/provaxxers/provaxxersTweets.csv', low_memory=False)\n",
    "\n",
    "#antivaxxers\n",
    "antivaxxersTweets = pd.read_csv('./datasets/antivaxxers/antivaxxersTweets.csv', low_memory=False)\n",
    "\n",
    "#antisinovaxxers\n",
    "#antisinovaxxersTweets = pd.read_csv('./datasets/antisinovaxxers/antisinovaxxersTweets.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawTweets = antivaxxersTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_date ='2020-02-29T00:00:00.000Z'\n",
    "#end_date = '2021-05-04T00:00:00.000Z'\n",
    "\n",
    "start_date ='2021-03-00T00:00:00.000Z'\n",
    "end_date = '2021-05-04T00:00:00.000Z'\n",
    "\n",
    "mask = (rawTweets['created_at'] > start_date) & (rawTweets['created_at'] <= end_date)\n",
    "filteredTweets = rawTweets.loc[mask]\n",
    "\n",
    "filteredTweets = filteredTweets.sort_values(['created_at']).reset_index().drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botometer\n",
    "rapid_api_key = ''\n",
    "twitter_app_auth = {\n",
    "    'consumer_key': '',\n",
    "    'consumer_secret': '',\n",
    "    'access_token': '',\n",
    "    'access_token_secret': '',\n",
    "  }\n",
    "bom = botometer.Botometer(wait_on_ratelimit=True,\n",
    "                          rapidapi_key=rapid_api_key,\n",
    "                          **twitter_app_auth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = bom.check_account(1255307463034843136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "import numpy as np \n",
    "from string import punctuation\n",
    "import unidecode\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "\n",
    "# Removendo hashtags, menções a usuários, numeros, termos curtos e links\n",
    "\n",
    "def proccess_text(raw_text):\n",
    "    \n",
    "    twitterData = pd.DataFrame(raw_text) \n",
    "    twitterData['processed_text'] = twitterData.text.str.replace(r'(http\\S+)', '') \\\n",
    "                                                    .str.replace(r'@[\\w]*', '') \\\n",
    "                                                    .str.replace(r'#[\\w]*','') \n",
    "\n",
    "    textWords = ' '.join([text for text in twitterData.processed_text])\n",
    "\n",
    "    # Removendo acentuação\n",
    "    textWords = [unidecode.unidecode(text) for text in twitterData.processed_text ]\n",
    "\n",
    "    # Criando lista com palavras e caracteres (stopwords) a serem removidos do texto\n",
    "    stopWords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "\n",
    "    # Separando a pontuação das palavras\n",
    "    punctSeparator = tokenize.WordPunctTokenizer()\n",
    "    punctuationList = list()\n",
    "    for punct in punctuation:\n",
    "        punctuationList.append(punct)\n",
    "\n",
    "    #personalList=['predictions','tomar cu']    \n",
    "\n",
    "    #stopWords =  stopWords + punctuationList + personalList\n",
    "    stopWords =   punctuationList + stopWords\n",
    "\n",
    "\n",
    "    # Iterando o texto removendo as stopwords\n",
    "    trasnformedText = list()\n",
    "    \n",
    "    for text in textWords:\n",
    "        newText = list()   \n",
    "        text = text.lower()\n",
    "        textWords = punctSeparator.tokenize(text)\n",
    "        for words in textWords:\n",
    "             if words not in stopWords:\n",
    "                #newText.append(stemmer.stem(words))\n",
    "                newText.append(words)\n",
    "        trasnformedText.append(' '.join(newText))\n",
    "    twitterData.processed_text = trasnformedText\n",
    "\n",
    "   \n",
    "    twitterData.processed_text = twitterData.processed_text.str.replace(r\"[^a-zA-Z#]\", \" \") \\\n",
    "                                                           .replace(r\"k\\k\", \" \") #\\\n",
    "                                                           #.apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3])) \n",
    "    \n",
    "    return twitterData.processed_text\n",
    "\n",
    "\n",
    "tweets = {'created_at': filteredTweets.created_at, 'text': filteredTweets.text,'author_id':filteredTweets.author_id}\n",
    "rawTweets = pd.DataFrame(tweets)\n",
    "\n",
    "rawTweets['processed_text'] = proccess_text(rawTweets.text)\n",
    "\n",
    "processedTweets = rawTweets.drop(columns=[\"text\"])\n",
    "\n",
    "trasnformedText = list()\n",
    "for phrase in processedTweets.processed_text:\n",
    "    newPhrase = list()   \n",
    "    newPhrase.append(' '.join(phrase.split()))\n",
    "    for words in newPhrase:\n",
    "        trasnformedText.append(''.join(newPhrase))\n",
    "processedTweets.processed_text = trasnformedText\n",
    "\n",
    "index=[x for x in processedTweets.index if processedTweets.processed_text[x].count(' ') < 3]\n",
    "processedTweets = processedTweets.drop(index)\n",
    "\n",
    "removeEmpty  = processedTweets.processed_text != ' '\n",
    "processedTweets = processedTweets[removeEmpty]\n",
    "\n",
    "processedTweets.reset_index(inplace=True)\n",
    "\n",
    "tweets = {'created_at': processedTweets.created_at, 'text': processedTweets.processed_text, 'author_id':processedTweets.author_id}\n",
    "docs = pd.DataFrame(tweets)\n",
    "\n",
    "docs = docs.sort_values(['created_at']).reset_index()\n",
    "\n",
    "docs = docs.drop(columns=[\"index\"])\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic\n",
    "\n",
    "<b>Custom sub-models</b>\n",
    "<ul>\n",
    "    <li>UMAP: Dimensionality reduction</li>\n",
    "    <li>HDBSCAN: Documents clustering</li>\n",
    "    <li>CountVectorizer: Extract all possible words</li>\n",
    "</ul> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():          \n",
    "    device = torch.device(\"cuda\")    \n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())    \n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sentence_transformers import SentenceTransformer, util\n",
    "#bert_model = SentenceTransformer(\"xlm-r-bert-base-nli-stsb-mean-tokens\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=10, min_dist=0.0, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import hdbscan\n",
    "\n",
    "#hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=30, \n",
    "#                                metric='euclidean',                      \n",
    "#                                cluster_selection_method='eom', prediction_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#vectorizer_model = CountVectorizer(ngram_range=(1, 2),min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic(#embedding_model=bert_model,\n",
    "                       language = \"multilingual\",\n",
    "                       top_n_words=10,\n",
    "                       n_gram_range=(1, 2),\n",
    "                       #min_topic_size=65,   \n",
    "                       #nr_topics = \"auto\",\n",
    "                       #umap_model=umap_model,\n",
    "                       #hdbscan_model=hdbscan_model,\n",
    "                       #vectorizer_model=vectorizer_model,\n",
    "                       low_memory=True,\n",
    "                       calculate_probabilities=False, \n",
    "                       verbose=True)#.fit(processedTweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(docs.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bertopic import BERTopic\n",
    "# Save model\n",
    "#topic_model.save(\"./models/antivaxxers_model\")\n",
    "\n",
    "# Load model\n",
    "#topic_model = BERTopic.load(\"./models/provaxxers_model\")\n",
    "#topics = topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = topic_model.get_topic_info(); freq.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_model.get_topic(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_model.visualize_hierarchy(top_n_topics=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similar_topics, similarity = topic_model.find_topics(\"doria\", top_n=30); similar_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs = {topic: [] for topic in set(topics)}\n",
    "for topic, doc in zip(topics, docs.text):\n",
    "    topic_docs[topic].append(doc)\n",
    "    \n",
    "    \n",
    "topic_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Reduction\n",
    "\n",
    "<ul>\n",
    "    <li>Manual Topic Reduction</li>\n",
    "    <li>Automatic Topic Reduction</li>\n",
    "    <li><b>Topic Reduction after Training</b></li>\n",
    "</ul> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTopics, newProbs = topic_model.reduce_topics(docs.text, topics, probs, nr_topics=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs = {topic: [] for topic in set(newTopics)}\n",
    "for topic, doc in zip(newTopics, docs.text):\n",
    "    topic_docs[topic].append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topic_docs[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_model.visualize_hierarchy(top_n_topics=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = topic_model.get_topic_info(); freq.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " topic_model.get_topic(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Topic Modeling\n",
    "\n",
    "[Dou et al. 2012] define um evento como ”uma ocorrencia que causa mudancas no volume de dados de texto que discutem o topico associado em um momento especifico. Logo, eventos podem ser vistos como um resumo sucinto dos fluxos de informacoes nas mıdias sociais, revelando a evolucao de fenomenos sociais ao longo de um determinado periodo de tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = docs.created_at.to_list()\n",
    "tweets = docs.text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(docs=tweets, \n",
    "                                                topics=newTopics, \n",
    "                                                #topics=topics,\n",
    "                                                timestamps=timestamps, \n",
    "                                                global_tuning=True,\n",
    "                                                evolution_tuning=True, \n",
    "                                                nr_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
