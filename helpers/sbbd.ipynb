{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtags mais mencionadas no Twitter\n",
    "\n",
    "<b>Argumento de pesquisa:</b> vacina vacinaco \n",
    "<br><b>Período de pesquisa:</b> 21/12/2020 á 26/03/2021\n",
    "\n",
    "<b>Pro-vaxxers</b>\n",
    "<ul>\n",
    "    <li>#vacinaja</li>\n",
    "    <li>#vacinaparatodos</li>\n",
    "    <li>#vemvacina</li>\n",
    "    <li>#vacinasim</li>\n",
    "    <li>#todospelasvacinas</li>\n",
    "</ul> \n",
    "\n",
    "<b>Anti-vaxxers</b>\n",
    "<ul>\n",
    "    <li>#vacinaobrigatorianao</li>\n",
    "    <li>#eunaovoutomarvacina</li>\n",
    "    <li>#naovoutomarvacina</li>\n",
    "    <li>#vacinanao</li>\n",
    "    <li>#vacinaobrigatorianunca</li>\n",
    "</ul> \n",
    "\n",
    "<b>Pro-sinovaxxers</b>\n",
    "<ul>\n",
    "    <li>#vachinanao</li>\n",
    "    <li>#todoscontravachina</li>\n",
    "    <li>#vacinadachinanao</li>\n",
    "    <li>#naoavachina</li>\n",
    "    <li>#naovoutomarvachina</li>\n",
    "</ul> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets = pd.read_csv('./datasets/tweets.csv',lineterminator='\\n', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np \n",
    "from string import punctuation\n",
    "import unidecode\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "\n",
    "    #tokens = tokenize.WordPunctTokenizer()\n",
    "    #tokens = tokenize.WhitespaceTokenizer()\n",
    "    #tokens = tokenize.MWETokenizer()\n",
    "\n",
    "def proccess_text(data_of_text, tk):\n",
    "    textWords = [unidecode.unidecode(text) for text in data_of_text]       \n",
    "    \n",
    "    \n",
    "    punctuationList = list()\n",
    "    for punct in punctuation:\n",
    "        if punct != '#':\n",
    "            punctuationList.append(punct)\n",
    "    trasnformedText = list()\n",
    "    \n",
    "    personalList=[\"...\",\"!#\",\"@#\",\"'#\",\".#\",\"\\\"#\",\"...#\",\"(#\",\"?#\",\"!!\"]  \n",
    "    punctuationList = punctuationList  \n",
    "    \n",
    "    for text in textWords:\n",
    "        newText = list()   \n",
    "        text = text.lower()\n",
    "        textWords = tk.tokenize(text)\n",
    "        for words in textWords: \n",
    "             if words not in punctuationList:\n",
    "                #newText.append(stemmer.stem(words))\n",
    "                 newText.append(words)\n",
    "        trasnformedText.append(' '.join(newText))\n",
    "    return trasnformedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "def count_hashtags(data_of_text):\n",
    "    regex = re.compile(r'#[\\w]*')\n",
    "    #regex = re.compile(r'#(\\w*[0-9a-zA-Z]+\\w*[0-9a-zA-Z])')\n",
    "\n",
    "    textWords = ' '.join([text for text in data_of_text])\n",
    "\n",
    "    hashtags = regex.findall(textWords)\n",
    "\n",
    "    hashtags = ' '.join([text for text in  hashtags])\n",
    "\n",
    "    tokenizing = tokenize.WhitespaceTokenizer()\n",
    "    tokenizedWords = tokenizing.tokenize(hashtags)\n",
    "    frequency = nltk.FreqDist(tokenizedWords)\n",
    "    df_frequency = pd.DataFrame({\"Hashtag\": list(frequency.keys()),\n",
    "                                       \"Frequency\": list(frequency.values())})\n",
    "\n",
    "    return df_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['processed_text'] = proccess_text(tweets.text, tokenize.WhitespaceTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = count_hashtags(tweets.processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.query('Hashtag.str.contains(\"eunaovou\") or Hashtag.str.contains(\"naovou\") or Hashtag.str.contains(\"inanao\")', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabloo\n",
    "tabloo.show(df)\n",
    "#tabloo.show(df.query('Hashtag.str.contains(\"eunaovou\") or Hashtag.str.contains(\"naovou\") or Hashtag.str.contains(\"inanao\")', engine='python'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequência individual por periodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#cpidacovid\n",
    "#cpidacovid\t = pd.read_csv('./datasets/cpi.csv', low_memory=False)\n",
    "\n",
    "\n",
    "#provaxxers\n",
    "vacinaja = pd.read_csv('./datasets/provaxxers/vacinaja.csv', low_memory=False)\n",
    "vacinasim = pd.read_csv('./datasets/provaxxers/vacinasim.csv', low_memory=False)\n",
    "vacinaparatodos = pd.read_csv('./datasets/provaxxers/vacinaparatodos.csv', low_memory=False)\n",
    "vemvacina = pd.read_csv('./datasets/provaxxers/vemvacina.csv', low_memory=False)\n",
    "todospelasvacinas = pd.read_csv('./datasets/provaxxers/todospelasvacinas.csv', low_memory=False)\n",
    "\n",
    "#antivaxxers\n",
    "vacinaobrigatorianao = pd.read_csv('./datasets/antivaxxers/vacinaobrigatorianao.csv', low_memory=False)\n",
    "vacinanao =  pd.read_csv('./datasets/antivaxxers/vacinanao.csv', low_memory=False)\n",
    "eunaovoutomarvacina =  pd.read_csv('./datasets/antivaxxers/eunaovoutomarvacina.csv', low_memory=False)\n",
    "naovoutomarvacina =  pd.read_csv('./datasets/antivaxxers/naovoutomarvacina.csv', low_memory=False)\n",
    "vacinaobrigatorianunca =  pd.read_csv('./datasets/antivaxxers/vacinaobrigatorianunca.csv', low_memory=False)\n",
    "\n",
    "#antisinovaxxers\n",
    "#vachinanao = pd.read_csv('./datasets/antisinovaxxers/vachinanao.csv', low_memory=False)\n",
    "#vachina = pd.read_csv('./datasets/antisinovaxxers/vachina.csv', low_memory=False)\n",
    "#naovoutomarvachina = pd.read_csv('./datasets/antisinovaxxers/naovoutomarvachina.csv', low_memory=False)\n",
    "#vacinadachinanao = pd.read_csv('./datasets/antisinovaxxers/vacinadachinanao.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtags_frequency(dataframe,hashtag,freq):\n",
    "    for text in dataframe.text:       \n",
    "        frequency = {'created_at': dataframe.created_at,'text': hashtag, 'frequency': 1}\n",
    "\n",
    "    hashtags_freq = pd.DataFrame(frequency)\n",
    "    hashtags_freq['created_at'] = pd.to_datetime(hashtags_freq['created_at'])\n",
    "    hashtags_freq = hashtags_freq.groupby('text').resample(freq, on='created_at').sum().reset_index().sort_values(by='created_at')\n",
    "    return hashtags_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpidacovid_hashtags = hashtags_frequency(cpidacovid,'#cpidacovid','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpidacovid_hashtags.to_csv('./datasets/req_cpidacovid.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pro-vaxxers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacinaja_hashtags = hashtags_frequency(vacinaja,'#provaxxers','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacinasim_hashtags =  hashtags_frequency(vacinasim,'#provaxxers','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacinaparatodos_hashtags =  hashtags_frequency(vacinaparatodos,'#provaxxers','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vemvacina_hashtags =  hashtags_frequency(vemvacina,'#provaxxers','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todospelasvacinas_hashtags =  hashtags_frequency(todospelasvacinas,'#provaxxers','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_allprovaxxers = pd.concat([vacinaja_hashtags, vacinasim_hashtags, vacinaparatodos_hashtags, vemvacina_hashtags, todospelasvacinas_hashtags], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_allprovaxxers.to_csv('./datasets/provaxxers/freq/freq_allprovaxxers.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provaxxersTweets = pd.concat([vacinaja, vacinasim, vacinaparatodos, vemvacina, todospelasvacinas], ignore_index=True)\n",
    "#provaxxersTweets.to_csv('./datasets/provaxxers/provaxxersTweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provaxxersTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provaxxersTweets.groupby(pd.Grouper(key='created_at',freq='1D')).sum().reset_index().sort_values(by='created_at')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anti-vaxxers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacinaobrigatorianao_hashtags = hashtags_frequency(vacinaobrigatorianao,'#antivaxxers','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacinanao_hashtags =  hashtags_frequency(vacinanao,'#antivaxxers','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eunaovoutomarvacina_hashtags = hashtags_frequency(eunaovoutomarvacina,'#antivaxxers','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naovoutomarvacina_hashtags =  hashtags_frequency(naovoutomarvacina,'#antivaxxers','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacinaobrigatorianunca_hashtags =  hashtags_frequency(vacinaobrigatorianunca,'#antivaxxers','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_allantivaxxers = pd.concat([vacinaobrigatorianao_hashtags, vacinanao_hashtags, eunaovoutomarvacina_hashtags, naovoutomarvacina_hashtags, vacinaobrigatorianunca_hashtags], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_allantivaxxers.to_csv('./datasets/antivaxxers/freq/freq_allantivaxxers.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#antivaxxersTweets = pd.concat([vacinaobrigatorianao, vacinanao, eunaovoutomarvacina, naovoutomarvacina, vacinaobrigatorianunca], ignore_index=True)\n",
    "#antivaxxersTweets.to_csv('./datasets/antivaxxers/antivaxxersTweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anti-sinovaxxers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vachinanao_hashtags = hashtags_frequency(vachinanao,'#antisinovaxxers','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vachina_hashtags =  hashtags_frequency(vachina,'#antisinovaxxers','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naovoutomarvachina_hashtags = hashtags_frequency(naovoutomarvachina,'#antisinovaxxers','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacinadachinanao_hashtags = hashtags_frequency(vacinadachinanao,'#antisinovaxxers','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_allantisinovaxxers = pd.concat([vachinanao_hashtags,vachina_hashtags,naovoutomarvachina_hashtags,vacinadachinanao_hashtags], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_allantisinovaxxers.to_csv('./datasets/antisinovaxxers/freq/freq_allantisinovaxxers.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antisinovaxxersTweets = pd.concat([vachinanao,vachina,naovoutomarvachina,vacinadachinanao], ignore_index=True)\n",
    "antisinovaxxersTweets.to_csv('./datasets/antisinovaxxers/antisinovaxxersTweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All-pandemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_allpandemic = pd.concat([freq_allprovaxxers, freq_allantivaxxers], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_allpandemic.to_csv('./datasets/freq_allpandemic.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_allpandemic.sort_values(['created_at']).reset_index().drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date ='2020-02-29T00:00:00.000Z'\n",
    "end_date = '2021-05-04T00:00:00.000Z'\n",
    "\n",
    "mask = (freq_allpandemic['created_at'] >= start_date) & (freq_allpandemic['created_at'] <= end_date)\n",
    "filteredTweets = freq_allpandemic.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredTweets.sort_values(['created_at']).reset_index().drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredTweets.to_csv('./datasets/freq_allpandemic.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisando peridos identificados a partir do gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agravamento da pandemia em Manaus\n",
    "# 16/01/2021 O Brasil registrou neste sábado (16/01), pelo quinta dia consecutivo, mais de mil mortes diárias ligadas à covid-19.\n",
    "# 17/01/2021 Vacinada enfermeira Mônica.\n",
    "# 17/01/2021 Anvisa aprova uso emergencial das vacinas CoronaVac e AstraZeneca no Brasil.\n",
    "\n",
    "# 15/01/2021 Em entrevista à Band, Bolsonaro chamou Doria de 'moleque'.\n",
    "#  'Fiz tudo o que estava ao meu alcance, o problema agora é do estado do Amazonas e da Prefeitura de Manaus'. \n",
    "#   Inacreditável. Inacreditável. Em outro país, isso talvez fosse classificado como genocídio. É um abandono aos brasileiros\", afirmou Doria.\n",
    "\n",
    "\n",
    "#start_date ='2021-01-08T00:00:00.000Z'\n",
    "#end_date = '2021-01-22T23:59:59.000Z'\n",
    "\n",
    "#--------------------------------------------------\n",
    "\n",
    "# 02/12/2020 Reino Unido começa a vacinar população\n",
    "# 13/12/2020 Campanha de vacinação para covid-19 começa nos EUA\n",
    "# 24/12/2020 Chile inicia vacinação com imunizante da Pfizer-BioNTech\n",
    "\n",
    "#start_date ='2020-11-19T00:00:00.000Z'\n",
    "#end_date = '2020-12-25T23:59:59.000Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date ='2020-11-19T00:00:00.000Z'\n",
    "end_date = '2020-12-25T23:59:59.000Z'\n",
    "\n",
    "mask = (vacinaobrigatorianao['created_at'] > start_date) & (vacinaobrigatorianao['created_at'] <= end_date)\n",
    "df = vacinaobrigatorianao.loc[mask]\n",
    "df\n",
    "#df.to_csv('teste.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np \n",
    "from string import punctuation\n",
    "import unidecode\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "\n",
    "# Removendo hashtags, menções a usuários, numeros, termos curtos e links\n",
    "\n",
    "def proccess_text(raw_text):\n",
    "    \n",
    "    twitterData = pd.DataFrame(raw_text) \n",
    "    twitterData['processed_text'] = twitterData.text.str.lower() \\\n",
    "                                                    .str.replace(r'(http\\S+)', '') \\\n",
    "                                                    .str.replace(r'@[\\w]*', '') \\\n",
    "                                                    .str.replace(r'#[\\w]*','') \n",
    "\n",
    "    textWords = ' '.join([text for text in twitterData.processed_text])\n",
    "\n",
    "    # Removendo acentuação\n",
    "    textWords = [unidecode.unidecode(text) for text in twitterData.processed_text ]\n",
    "\n",
    "    # Criando lista com palavras e caracteres (stopwords) a serem removidos do texto\n",
    "    stopWords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "\n",
    "    # Separando a pontuação das palavras\n",
    "    punctSeparator = tokenize.WordPunctTokenizer()\n",
    "    punctuationList = list()\n",
    "    for punct in punctuation:\n",
    "        punctuationList.append(punct)\n",
    "\n",
    "    personalList=['pra','predictions']    \n",
    "\n",
    "    stopWords =  stopWords + punctuationList + personalList\n",
    "\n",
    "    # Iterando o texto removendo as stopwords\n",
    "    trasnformedText = list()\n",
    "\n",
    "    for text in textWords:\n",
    "        newText = list()   \n",
    "        #text = text.lower()\n",
    "        textWords = punctSeparator.tokenize(text)\n",
    "        for words in textWords:\n",
    "            if words not in stopWords:\n",
    "                #newText.append(stemmer.stem(words))\n",
    "                newText.append(words)\n",
    "        trasnformedText.append(' '.join(newText))\n",
    "    twitterData.processed_text = trasnformedText\n",
    "\n",
    "    twitterData.processed_text = twitterData.processed_text.str.replace(r\"[^a-zA-Z#]\", \" \") \\\n",
    "                                                               .replace(r\"k\\k\", \" \") \\\n",
    "                                                               .apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "    \n",
    "    return twitterData.processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = proccess_text(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "\n",
    "textWords = ' '.join([text for text in  df.processed_text ])\n",
    "\n",
    "wordCloud = WordCloud(width= 800, height= 600,  #WordCloud com a lista de palavas\n",
    "                          max_font_size = 110,\n",
    "                          collocations = False).generate(textWords)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordCloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF / LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "def corpus_generate(data):\n",
    "   \n",
    "    def preprocess(text):\n",
    "        result = []\n",
    "        for token in gensim.utils.simple_preprocess(text):        \n",
    "                result.append(token)\n",
    "        return result\n",
    "\n",
    "    processed_docs = data.processed_text.map(preprocess)\n",
    "\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    \n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "    \n",
    "    return processed_docs, dictionary, corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs, dictionary, corpus_tfidf = corpus_generate(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opcional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in tqdm(range(start, limit, step)):\n",
    "        model = LdaMulticore(corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "# Colocando parametros na função\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus_tfidf, texts=processed_docs, start=1, limit=30, step=2)\n",
    "# Mostrando visualmente a quantidade de tópicos\n",
    "limit=30; start=1; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Tópicos\")\n",
    "plt.ylabel(\"Score de Coerência\")\n",
    "plt.legend((\"Valores de Coerência\"), loc='best')\n",
    "plt.show()\n",
    "# Lista dos valores de coerência, para melhor identificar o ponto de inflexão do gráfico\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"A quantidade de tópicos =\", m, \" tem um valor de coerência de \", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA #VacinaJa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vacinaja = LdaMulticore(corpus_tfidf, id2word=dictionary, num_topics=4 , passes=40,chunksize = 1000, workers=4, per_word_topics=True,\n",
    "                        alpha = 0.9,\n",
    "                        eta = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_vacinaja.print_topics(num_words = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda  = CoherenceModel(model=lda_vacinaja, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nScore de Coerência: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "\n",
    "cloud = WordCloud( background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=20,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "topics = lda_vacinaja.show_topics(formatted=False)\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15,15), sharex=True, sharey=True)\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=600)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_vacinaja, corpus_tfidf, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
