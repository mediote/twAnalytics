{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets = pd.read_csv('./datasets/all/tweets_old.csv', low_memory=False)\n",
    "tweets5 = pd.read_csv('./datasets/all/tweets_5.csv', low_memory=False)\n",
    "tweets6 = pd.read_csv('./datasets/all/tweets_6.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {'id': tweets.id,'created_at': tweets.created_at,'author_id': tweets.author_id,'text': tweets.text}\n",
    "df1 = pd.DataFrame(data1)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = {'id': tweets5.id,'created_at': tweets5.created_at,'author_id': tweets5.author_id,'text': tweets5.text}\n",
    "df2 = pd.DataFrame(data2)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = {'id': tweets6.id,'created_at': tweets6.created_at,'author_id': tweets6.author_id,'text': tweets6.text}\n",
    "df3 = pd.DataFrame(data3)\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2.append(df3,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.append(df1,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv('./datasets/tweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./datasets/tweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('todospelasvacinas.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date ='2021-03-25T02:00:00.000Z'\n",
    "end_date = '2021-03-25T02:30:00.000Z'\n",
    "\n",
    "mask = (tweets['created_at'] > start_date) & (tweets['created_at'] <= end_date)\n",
    "df = tweets.loc[mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets = pd.read_csv('./datasets/tweets.csv',lineterminator='\\n', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np \n",
    "from string import punctuation\n",
    "import unidecode\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "\n",
    "    #tokens = tokenize.WordPunctTokenizer()\n",
    "    #tokens = tokenize.WhitespaceTokenizer()\n",
    "    #tokens = tokenize.MWETokenizer()\n",
    "\n",
    "def proccess_text(data_of_text, tk):\n",
    "    textWords = [unidecode.unidecode(text) for text in data_of_text]       \n",
    "    \n",
    "    \n",
    "    punctuationList = list()\n",
    "    for punct in punctuation:\n",
    "        if punct != '#':\n",
    "            punctuationList.append(punct)\n",
    "    trasnformedText = list()\n",
    "    \n",
    "    personalList=[\"...\",\"!#\",\"@#\",\"'#\",\".#\",\"\\\"#\",\"...#\",\"(#\",\"?#\",\"!!\"]  \n",
    "    punctuationList = punctuationList  \n",
    "    \n",
    "    for text in textWords:\n",
    "        newText = list()   \n",
    "        text = text.lower()\n",
    "        textWords = tk.tokenize(text)\n",
    "        for words in textWords: \n",
    "             if words not in punctuationList:\n",
    "                #newText.append(stemmer.stem(words))\n",
    "                 newText.append(words)\n",
    "        trasnformedText.append(' '.join(newText))\n",
    "    return trasnformedText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "def count_hashtags(data_of_text):\n",
    "    regex = re.compile(r'#[\\w]*')\n",
    "    #regex = re.compile(r'#(\\w*[0-9a-zA-Z]+\\w*[0-9a-zA-Z])')\n",
    "\n",
    "    textWords = ' '.join([text for text in data_of_text])\n",
    "\n",
    "    hashtags = regex.findall(textWords)\n",
    "\n",
    "    hashtags = ' '.join([text for text in  hashtags])\n",
    "\n",
    "    tokenizing = tokenize.WhitespaceTokenizer()\n",
    "    tokenizedWords = tokenizing.tokenize(hashtags)\n",
    "    frequency = nltk.FreqDist(tokenizedWords)\n",
    "    df_frequency = pd.DataFrame({\"Hashtag\": list(frequency.keys()),\n",
    "                                       \"Frequency\": list(frequency.values())})\n",
    "\n",
    "    return df_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets['processed_text'] = proccess_text(tweets.text, tokenize.WhitespaceTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localizando Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "#regex = re.compile(r'#eunaovoutomarvacina[\\w]*')\n",
    "#regex = re.compile(r'#[\\w]*')\n",
    "\n",
    "token = tokenize.WhitespaceTokenizer()\n",
    "#token = tokenize.WordPunctTokenizer()\n",
    "#trasnformedText = list()\n",
    "\n",
    "for text in tweets.processed_text:\n",
    "    newText = list()   \n",
    "    tokens =  token.tokenize(text)\n",
    "    for text in tokens:\n",
    "         #if regex.findall(text):\n",
    "        #if text == \"#todospelasvacinas\":\n",
    "        #newText.append(\"#eunaovoutomarvacina\")\n",
    "            #newText.append(text)\n",
    "            #freq = newText.count(\"#eunaovoutomarvacina\")\n",
    "    #trasnformedText.append(' '.join(newText))\n",
    "    #data = {'created_at': tweets.created_at,'text': trasnformedText, 'frequency': freq}\n",
    "     data = {'created_at': tweets.created_at,'text': \"#eunaovoutomarvacina\", 'frequency': 1}\n",
    "#tweets['hashtag'] = trasnformedText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.shape\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created_at'] = pd.to_datetime(df['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('text').resample('D', on='created_at').sum().reset_index().sort_values(by='created_at')\n",
    "df.to_csv('freq_eunaovoutoarvacina.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(pd.Grouper(key='created_at',freq='7D')).sum().reset_index().sort_values(by='created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.text =  proccess_text(df.text,tokenize.WordPunctTokenizer())\n",
    "df.to_csv('freq_eunaovoutoarvacina.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtags mais mencionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = count_hashtags(tweets.processed_text)\n",
    "df.query('Hashtag.str.contains(\"eunao\") or Hashtag.str.contains(\"naovou\") or Hashtag.str.contains(\"obrigatoria\") or Hashtag.str.contains(\"inanao\")', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabloo\n",
    "tabloo.show(df.query('Hashtag.str.contains(\"eunao\") or Hashtag.str.contains(\"naovou\") or Hashtag.str.contains(\"obrigatoria\") or Hashtag.str.contains(\"inanao\")', engine='python'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro-vaxxers\n",
    "\t#vacinaja - 14138\n",
    "\t#vacinaparatodos - 12043\n",
    "\t#vemvacina - 11213\n",
    "\t#vacinasim - 3466\n",
    "\t#todospelasvacinas - 2121\n",
    "Anti-vaxxers\n",
    "\t#vacinaobrogatÃ³rianao - 341\n",
    "\t#eunaovoutomarvacina - 198\n",
    "\t#naovoutomarvacina - 106\n",
    "\t#vacinanao - 95\t\n",
    "\t#vacinaobrigatorianunca - 20\n",
    "Anti-sinovaxxers\n",
    "\t#vachinanao - 147\n",
    "\t#todoscontravachina - 8\n",
    "\t#vacinadachinanao - 6\n",
    "\t#naoavachina - 4\n",
    "\t#naovoutomarvachina - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
