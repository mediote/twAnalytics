{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data crawling on Twitter: Full-archive search \n",
    "\n",
    "Documentation: https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all \n",
    "\n",
    "Endpoint URL: https://api.twitter.com/2/tweets/search/all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "query='((vacina%20vacinacao)%20OR%20(vacina%20OR%20vacinacao))%20-rt'\n",
    "start_time='2020-02-29T00%3A00%3A00Z'\n",
    "end_time='2021-07-12T00%3A00%3A00Z'\n",
    "\n",
    "load_dotenv()\n",
    "auth_token = os.environ.get('AUTH_TOKEN')\n",
    "header = {'Authorization': 'Bearer ' + auth_token}\n",
    "\n",
    "max_results='500'\n",
    "next_token=''\n",
    "\n",
    "url='https://api.twitter.com/2/tweets/search/all?query='+query+'&start_time='+start_time+'&end_time='+end_time+'&max_results='+max_results+'&expansions=author_id&tweet.fields=created_at'\n",
    "response = requests.get(url,headers=header)\n",
    "time.sleep(1)\n",
    "listOfTweets = json.loads(response.content)\n",
    "print('New Request on',url)\n",
    "\n",
    "tweets = pd.DataFrame(listOfTweets['data'])   \n",
    "\n",
    "if 'next_token' in listOfTweets['meta']:    \n",
    "    next_token = listOfTweets['meta']['next_token']   \n",
    "    \n",
    "    while 'next_token' in listOfTweets['meta']:        \n",
    "        url='https://api.twitter.com/2/tweets/search/all?query='+query+'&start_time='+start_time+'&end_time='+end_time+'&max_results='+max_results+'&next_token='+next_token+'&expansions=author_id&tweet.fields=created_at'\n",
    "        response = requests.get(url,headers=header)  \n",
    "        time.sleep(1)\n",
    "        listOfTweets = json.loads(response.content)         \n",
    "       \n",
    "        print('New Request on',url)\n",
    "        \n",
    "        if 'data' in listOfTweets:\n",
    "            tweets = tweets.append(pd.DataFrame(listOfTweets['data']),ignore_index=True)\n",
    "\n",
    "            if  'meta' in listOfTweets:         \n",
    "                if 'next_token' in listOfTweets['meta']:\n",
    "                    next_token =  listOfTweets['meta']['next_token']\n",
    "                else:\n",
    "                    print('Done! Total of ', len(tweets), 'tweets collected.')                \n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            print('Missing request')\n",
    "            break\n",
    "else:\n",
    "    tweets = pd.DataFrame(listOfTweets['data'])\n",
    "\n",
    "    print('Done! Total of', len(tweets), 'tweets collected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterData.to_csv('./tweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#provaxxers\n",
    "provaxxers = pd.read_csv('./datasets/provaxxers.csv', low_memory=False)\n",
    "\n",
    "#antivaxxers\n",
    "#antivaxxers = pd.read_csv('./datasets/provaxxers.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawTweets = provaxxersTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date ='2020-02-29T00:00:00.000Z'\n",
    "end_date = '2021-05-04T00:00:00.000Z'\n",
    "\n",
    "#start_date ='2021-03-00T00:00:00.000Z'\n",
    "#end_date = '2021-05-04T00:00:00.000Z'\n",
    "\n",
    "mask = (rawTweets['created_at'] > start_date) & (rawTweets['created_at'] <= end_date)\n",
    "filteredTweets = rawTweets.loc[mask]\n",
    "\n",
    "filteredTweets = filteredTweets.sort_values(['created_at']).reset_index().drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "import numpy as np \n",
    "from string import punctuation\n",
    "import unidecode\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "\n",
    "# Removendo hashtags, menções a usuários, numeros, termos curtos e links\n",
    "\n",
    "def proccess_text(raw_text):\n",
    "    \n",
    "    twitterData = pd.DataFrame(raw_text) \n",
    "    twitterData['processed_text'] = twitterData.text.str.replace(r'(http\\S+)', '') \\\n",
    "                                                    .str.replace(r'@[\\w]*', '') \\\n",
    "                                                    .str.replace(r'#[\\w]*','') \n",
    "\n",
    "    textWords = ' '.join([text for text in twitterData.processed_text])\n",
    "\n",
    "    # Removendo acentuação\n",
    "    textWords = [unidecode.unidecode(text) for text in twitterData.processed_text ]\n",
    "\n",
    "    # Criando lista com palavras e caracteres (stopwords) a serem removidos do texto\n",
    "    stopWords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "\n",
    "    # Separando a pontuação das palavras\n",
    "    punctSeparator = tokenize.WordPunctTokenizer()\n",
    "    punctuationList = list()\n",
    "    for punct in punctuation:\n",
    "        punctuationList.append(punct)\n",
    "\n",
    "    #personalList=['predictions','tomar cu']    \n",
    "\n",
    "    #stopWords =  stopWords + punctuationList + personalList\n",
    "    stopWords =   punctuationList + stopWords\n",
    "\n",
    "\n",
    "    # Iterando o texto removendo as stopwords\n",
    "    trasnformedText = list()\n",
    "    \n",
    "    for text in textWords:\n",
    "        newText = list()   \n",
    "        text = text.lower()\n",
    "        textWords = punctSeparator.tokenize(text)\n",
    "        for words in textWords:\n",
    "             if words not in stopWords:\n",
    "                #newText.append(stemmer.stem(words))\n",
    "                newText.append(words)\n",
    "        trasnformedText.append(' '.join(newText))\n",
    "    twitterData.processed_text = trasnformedText\n",
    "\n",
    "   \n",
    "    twitterData.processed_text = twitterData.processed_text.str.replace(r\"[^a-zA-Z#]\", \" \") \\\n",
    "                                                           .replace(r\"k\\k\", \" \") #\\\n",
    "                                                           #.apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3])) \n",
    "    \n",
    "    return twitterData.processed_text\n",
    "\n",
    "\n",
    "tweets = {'created_at': filteredTweets.created_at, 'text': filteredTweets.text,'author_id':filteredTweets.author_id}\n",
    "rawTweets = pd.DataFrame(tweets)\n",
    "\n",
    "rawTweets['processed_text'] = proccess_text(rawTweets.text)\n",
    "\n",
    "processedTweets = rawTweets.drop(columns=[\"text\"])\n",
    "\n",
    "trasnformedText = list()\n",
    "for phrase in processedTweets.processed_text:\n",
    "    newPhrase = list()   \n",
    "    newPhrase.append(' '.join(phrase.split()))\n",
    "    for words in newPhrase:\n",
    "        trasnformedText.append(''.join(newPhrase))\n",
    "processedTweets.processed_text = trasnformedText\n",
    "\n",
    "index=[x for x in processedTweets.index if processedTweets.processed_text[x].count(' ') < 3]\n",
    "processedTweets = processedTweets.drop(index)\n",
    "\n",
    "removeEmpty  = processedTweets.processed_text != ' '\n",
    "processedTweets = processedTweets[removeEmpty]\n",
    "\n",
    "processedTweets.reset_index(inplace=True)\n",
    "\n",
    "tweets = {'created_at': processedTweets.created_at, 'text': processedTweets.processed_text, 'author_id':processedTweets.author_id}\n",
    "docs = pd.DataFrame(tweets)\n",
    "\n",
    "docs = docs.sort_values(['created_at']).reset_index()\n",
    "\n",
    "docs = docs.drop(columns=[\"index\"])\n",
    "\n",
    "docs.to_csv('./datasets/kdmile/provaxxers.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
