{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a45be2",
   "metadata": {},
   "source": [
    "# 1-  Data crawling on TwitterAPI: Full-archive search \n",
    "\n",
    "Documentation: https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all \n",
    "\n",
    "Endpoint URL: https://api.twitter.com/2/tweets/search/all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e68fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "auth_token = os.environ.get('AUTH_TOKEN')\n",
    "header = {'Authorization': 'Bearer '+auth_token}\n",
    "\n",
    "class TwitterHook():\n",
    "\n",
    "    def __init__(self, query, header = None, start_time = None, end_time = None, max_results= None):\n",
    "        self.query = query\n",
    "        self.header = header\n",
    "        self.start_time = '2020-02-29T00%3A00%3A00Z'\n",
    "        self.end_time = '2021-05-04T00%3A00%3A00Z'\n",
    "        self.max_results = '500'\n",
    "\n",
    "    def create_url(self):\n",
    "        query = self.query\n",
    "        tweet_fields = \"tweet.fields=author_id,id,created_at,in_reply_to_user_id,text\"\n",
    "        user_fields = \"expansions=author_id&user.fields=id,name,username,created_at\"\n",
    "        start_time = (\n",
    "            f\"&start_time={self.start_time}\"\n",
    "            if self.start_time\n",
    "            else \"\"\n",
    "        )\n",
    "        end_time = (\n",
    "            f\"&end_time={self.end_time}\"\n",
    "            if self.end_time\n",
    "            else \"\"\n",
    "        )\n",
    "        max_results  = (\n",
    "            f\"&max_results={self.max_results}\"\n",
    "            if self.max_results\n",
    "            else \"\"\n",
    "        )\n",
    "        url = \"https://api.twitter.com/2/tweets/search/all?query={}&{}&{}{}{}{}\".format(\n",
    "               query, tweet_fields, user_fields, start_time, end_time, max_results\n",
    "        )\n",
    "        return url\n",
    "\n",
    "    def connect_to_endpoint(self, url, header):\n",
    "        response = requests.get(url,headers=header)\n",
    "        listOfTweets = json.loads(response.content)\n",
    "        return  listOfTweets\n",
    "\n",
    "\n",
    "    def paginate(self, url, header, next_token=\"\"):\n",
    "        if next_token:\n",
    "            full_url = f\"{url}&next_token={next_token}\"\n",
    "            print('New Request on',full_url)\n",
    "        else:\n",
    "            full_url = url\n",
    "            print('New Request on',full_url)\n",
    "        data = self.connect_to_endpoint(full_url, header)\n",
    "        yield data\n",
    "        if \"next_token\" in data.get(\"meta\", {}):\n",
    "            yield from self.paginate(url, header, data['meta']['next_token'])\n",
    "\n",
    "\n",
    "    def run(self):  \n",
    "        url = self.create_url()\n",
    "        yield from self.paginate(url, header)\n",
    "        \n",
    "        \n",
    "def GetTweets(query):\n",
    "    tweets = pd.DataFrame()\n",
    "    for pg in TwitterHook(query).run():\n",
    "        time.sleep(1)  \n",
    "        \n",
    "        if 'data' in pg:\n",
    "            tweets =  tweets.append(pg['data'],ignore_index=True)\n",
    "        else:\n",
    "             print('Missing request')\n",
    "        \n",
    "    print('Done! Total of', len(tweets), 'tweets collected.')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a16896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = GetTweets('%23vacinaobrigatorianunca%20-rt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a222cf05",
   "metadata": {},
   "source": [
    "# 2- Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae16ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np \n",
    "from string import punctuation\n",
    "import unidecode\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "\n",
    "\n",
    "def proccess_text(tweets):\n",
    "    \n",
    "    # Removing links, mentions and hashtags\n",
    "    tweets['processed_text'] = tweets.text.str.replace(r'(http\\S+)', '') \\\n",
    "                                          .str.replace(r'@[\\w]*', '') \\\n",
    "                                          .str.replace(r'#[\\w]*','') \n",
    "    print('[ok] - Removing links.')\n",
    "    print('[ok] - Removing mentions.')\n",
    "    print('[ok] - Removing hashtags.')\n",
    "\n",
    "    textWords = ' '.join([text for text in tweets.processed_text])\n",
    "\n",
    "    # Removing accent\n",
    "    textWords = [unidecode.unidecode(text) for text in tweets.processed_text ]    \n",
    "    print('[ok] - Removing accent.')\n",
    "    \n",
    "    # Creating a list of words and characters (stopwords) to be removed from the text\n",
    "    stopWords = nltk.corpus.stopwords.words(\"portuguese\")    \n",
    "    print('[ok] - Creating a list of words and characters (stopwords) to be removed from the text.')\n",
    "    \n",
    "    \n",
    "    # Separating punctuation from words\n",
    "    punctSeparator = tokenize.WordPunctTokenizer()\n",
    "    punctuationList = list()\n",
    "    for punct in punctuation:\n",
    "        punctuationList.append(punct)\n",
    "        \n",
    "    stopWords =   punctuationList + stopWords    \n",
    "    print('[ok] - Separating punctuation from words.')\n",
    "\n",
    "\n",
    "    # Iterating over the text and removing stop words \n",
    "    trasnformedText = list()    \n",
    "    for text in textWords:\n",
    "        newText = list()   \n",
    "        text = text.lower()\n",
    "        textWords = punctSeparator.tokenize(text)\n",
    "        for words in textWords:\n",
    "             if words not in stopWords:\n",
    "                #newText.append(stemmer.stem(words))\n",
    "                newText.append(words)\n",
    "        trasnformedText.append(' '.join(newText))\n",
    "    tweets.processed_text = trasnformedText\n",
    "    print('[ok] - Iterating over the text and removing stop words.')\n",
    "   \n",
    "    # Removing all non-text characters\n",
    "    tweets.processed_text = tweets.processed_text.str.replace(r\"[^a-zA-Z#]\", \" \")                                                         \n",
    "    print('[ok] - Removing all non-text characters.')\n",
    "   \n",
    "    trasnformedText = list()\n",
    "    for phrase in tweets.processed_text:\n",
    "        newPhrase = list()   \n",
    "        newPhrase.append(' '.join(phrase.split()))\n",
    "        for words in newPhrase:\n",
    "            trasnformedText.append(''.join(newPhrase))\n",
    "    tweets.processed_text = trasnformedText\n",
    "    \n",
    "    # Removing tweets with less than three terms\n",
    "    index=[x for x in tweets.index if tweets.processed_text[x].count(' ') < 3]\n",
    "    tweets = tweets.drop(index)\n",
    "    print('[ok] - Removing tweets with less than three terms.')\n",
    "\n",
    "    # Removing empty lines\n",
    "    removeEmpty  = tweets.processed_text != ' '\n",
    "    tweets = tweets[removeEmpty]\n",
    "    print('[ok] - Removing empty lines.')\n",
    "\n",
    "    tweets.reset_index(inplace=True)\n",
    "    tweets = {'created_at': tweets.created_at, 'id':tweets.id,'author_id':tweets.author_id,'in_reply_to_user_id':tweets.in_reply_to_user_id, 'text': tweets.processed_text}\n",
    "    tweets = pd.DataFrame(tweets)\n",
    "    tweets = tweets.sort_values(['created_at']).reset_index().drop(columns=[\"index\"])\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c759d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = proccess_text(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd2cf6c",
   "metadata": {},
   "source": [
    "# 3- Topic Modeling with BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6509094",
   "metadata": {},
   "source": [
    "### Checking GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5968c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():        \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")    \n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())    \n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c689244",
   "metadata": {},
   "source": [
    "### Initializing BERTopic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a980c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tweets\n",
    "\n",
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic(language = 'multilingual',\n",
    "                       #embedding_model=bert_model,\n",
    "                       top_n_words=10,\n",
    "                       n_gram_range=(1, 2),\n",
    "                       min_topic_size=50,   \n",
    "                       nr_topics = 'auto',\n",
    "                       #umap_model=umap_model,                      \n",
    "                       low_memory=True,\n",
    "                       calculate_probabilities=False, \n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4799676e",
   "metadata": {},
   "source": [
    "### Generating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5825bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(docs.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f147bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b39ab",
   "metadata": {},
   "source": [
    "### Reducing the number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df7dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "newTopics, newProbs = topic_model.reduce_topics(docs.text, topics, probs, nr_topics=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392913af",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa33064",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_representative_docs(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a073e5ab",
   "metadata": {},
   "source": [
    "### Dynamic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6848f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = docs.created_at.to_list()\n",
    "tweets = docs.text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(docs=tweets, \n",
    "                                                topics=newTopics,                                                                                           \n",
    "                                                timestamps=timestamps, \n",
    "                                                global_tuning=True,\n",
    "                                                evolution_tuning=True, \n",
    "                                                nr_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
